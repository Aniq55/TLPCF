{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from functools import reduce\n",
    "import operator\n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time_start = {\n",
    "    \"reddit\": 2261813.658,\n",
    "    \"Contacts\": 2047800,\n",
    "    \"wikipedia\": 2218288.6,\n",
    "    \"uci\": 6714558.3,\n",
    "    \"SocialEvo\": 18711359,\n",
    "    \"mooc\": 2250151.6,\n",
    "    \"lastfm\": 120235473,\n",
    "    \"enron\": 93431801,\n",
    "    \"Flights\": 106,\n",
    "    \"UNvote\": 2019686400,\n",
    "    \"CanParl\": 347155200,\n",
    "    \"USLegis\": 10,\n",
    "    \"UNtrade\": 883612800\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_ATD(E, E_, dataset):\n",
    "    E_test = E[E['ts']>test_time_start[dataset]]\n",
    "    E_fake_test = E_[E_['ts']>test_time_start[dataset]]\n",
    "    \n",
    "    T = np.max(np.array(E_test['ts'])) - np.min(np.array(E_test['ts']))\n",
    "    n = len(E_test)\n",
    "    \n",
    "    Q = 0.0\n",
    "    for index, row in E_test.iterrows():\n",
    "        u = row['u']\n",
    "        v = row['i']\n",
    "        t = row['ts']\n",
    "        \n",
    "        T_uv = np.array(E_fake_test[(E_fake_test['u']==u)&(E_fake_test['i']==v)]['ts'])\n",
    "        \n",
    "        Q += np.min(np.abs(T_uv -t))\n",
    "        \n",
    "    return Q/(n*T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_ACD(E, E_, dataset):\n",
    "    E_test = E[E['ts']>test_time_start[dataset]]\n",
    "    E_fake_test = E_[E_['ts']>test_time_start[dataset]]\n",
    "    \n",
    "    T = np.max(np.array(E_test['ts'])) - np.min(np.array(E_test['ts']))\n",
    "    n = len(E_test)\n",
    "    T_bar = T/n\n",
    "    \n",
    "    Q = 0.0\n",
    "    for index, row in E_test.iterrows():\n",
    "        u = row['u']\n",
    "        v = row['i']\n",
    "        t = row['ts']\n",
    "        \n",
    "        count_real = len(E_test[(E_test['u']==u)&(E_test['i']==v)&(E_test['ts']>t-T_bar)&(E_test['ts']<t+T_bar)])\n",
    "        count_fake = len(E_fake_test[(E_fake_test['u']==u)&(E_fake_test['i']==v)&(E_fake_test['ts']>t-T_bar)&(E_fake_test['ts']<t+T_bar)])\n",
    "        \n",
    "        Q += np.abs(count_real - count_fake)\n",
    "    \n",
    "    return Q/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ \"wikipedia\", \"reddit\", \"uci\"]\n",
    "distortions = ['intense_5', 'shuffle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [16:40<00:00, 100.10s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets[1]\n",
    "distortion = distortions[1]\n",
    "E_real = pd.read_csv(f\"/home/chri6578/Documents/gttp/data/{dataset}/ml_{dataset}.csv\") \n",
    "\n",
    "ATD_all = []\n",
    "ACD_all = []\n",
    "for sample in tqdm(range(1,11)):\n",
    "    E_distort = pd.read_csv(f\"/home/chri6578/Documents/gttp/data/{dataset}/{distortion}_{sample}_ml_{dataset}.csv\")\n",
    "    ATD_all.append(f_ATD(E_real, E_distort, dataset))\n",
    "    ACD_all.append(f_ACD(E_real, E_distort, dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09971171953691128,\n",
       " 0.09839795047943865,\n",
       " 0.09912133218912875,\n",
       " 0.09906911349777363,\n",
       " 0.09900999985162606,\n",
       " 0.0985220263954334,\n",
       " 0.09937001318235185,\n",
       " 0.09902831996743215,\n",
       " 0.09852367890855039,\n",
       " 0.0985590201865276]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATD_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0329344582469986,\n",
       " 1.032825403749492,\n",
       " 1.03297411442791,\n",
       " 1.0331823093776953,\n",
       " 1.0330137706088216,\n",
       " 1.033063340834961,\n",
       " 1.0331624812872395,\n",
       " 1.0330236846540495,\n",
       " 1.0330038565635937,\n",
       " 1.0328848880208592]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACD_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence95(metric_list):\n",
    "    # Step 1: Calculate the mean\n",
    "    mean = np.mean(metric_list)\n",
    "    # Step 2: Calculate the standard error of the mean (SEM)\n",
    "    sem = stats.sem(metric_list)\n",
    "    # Step 3: Find the critical value for a 95% confidence interval\n",
    "    confidence_level = 0.95\n",
    "    degrees_of_freedom = len(metric_list) - 1\n",
    "    critical_value = stats.t.ppf((1 + confidence_level) / 2, degrees_of_freedom)\n",
    "\n",
    "    # Step 4: Calculate the margin of error\n",
    "    margin_of_error = critical_value * sem\n",
    "    return mean, margin_of_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09893131741951737, 0.00030433736045848805)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence95(ATD_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.033006830777162, 8.003200026644353e-05)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence95(ACD_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
