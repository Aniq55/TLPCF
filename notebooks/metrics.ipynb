{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from functools import reduce\n",
    "import operator\n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time_start = {\n",
    "    \"reddit\": 2261813.658,\n",
    "    \"Contacts\": 2047800,\n",
    "    \"wikipedia\": 2218288.6,\n",
    "    \"uci\": 6714558.3,\n",
    "    \"SocialEvo\": 18711359,\n",
    "    \"mooc\": 2250151.6,\n",
    "    \"lastfm\": 120235473,\n",
    "    \"enron\": 93431801,\n",
    "    \"Flights\": 106,\n",
    "    \"UNvote\": 2019686400,\n",
    "    \"CanParl\": 347155200,\n",
    "    \"USLegis\": 10,\n",
    "    \"UNtrade\": 883612800\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_ATD(E, E_, dataset):\n",
    "    E_test = E[E['ts']>test_time_start[dataset]]\n",
    "    E_fake_test = E_[E_['ts']>test_time_start[dataset]]\n",
    "    \n",
    "    T = np.max(np.array(E_test['ts'])) - np.min(np.array(E_test['ts']))\n",
    "    n = len(E_test)\n",
    "    \n",
    "    Q = 0.0\n",
    "    for index, row in E_test.iterrows():\n",
    "        u = row['u']\n",
    "        v = row['i']\n",
    "        t = row['ts']\n",
    "        \n",
    "        T_uv = np.array(E_fake_test[(E_fake_test['u']==u)&(E_fake_test['i']==v)]['ts'])\n",
    "        \n",
    "        Q += np.min(np.abs(T_uv -t))\n",
    "        \n",
    "    return Q/(n*T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_ACD(E, E_, dataset):\n",
    "    E_test = E[E['ts']>test_time_start[dataset]]\n",
    "    E_fake_test = E_[E_['ts']>test_time_start[dataset]]\n",
    "    \n",
    "    T = np.max(np.array(E_test['ts'])) - np.min(np.array(E_test['ts']))\n",
    "    n = len(E_test)\n",
    "    T_bar = T/n\n",
    "    \n",
    "    Q = 0.0\n",
    "    for index, row in E_test.iterrows():\n",
    "        u = row['u']\n",
    "        v = row['i']\n",
    "        t = row['ts']\n",
    "        \n",
    "        count_real = len(E_test[(E_test['u']==u)&(E_test['i']==v)&(E_test['ts']>t-T_bar)&(E_test['ts']<t+T_bar)])\n",
    "        count_fake = len(E_fake_test[(E_fake_test['u']==u)&(E_fake_test['i']==v)&(E_fake_test['ts']>t-T_bar)&(E_fake_test['ts']<t+T_bar)])\n",
    "        \n",
    "        Q += np.abs(count_real - count_fake)\n",
    "    \n",
    "    return Q/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ \"wikipedia\", \"reddit\", \"uci\", \"lastfm\", \"mooc\"]\n",
    "distortions = ['intense_5', 'shuffle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [43:48<00:00, 262.83s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets[3]\n",
    "distortion = distortions[1]\n",
    "E_real = pd.read_csv(f\"/home/chri6578/Documents/gttp/data/{dataset}/ml_{dataset}.csv\") \n",
    "\n",
    "ATD_all = []\n",
    "ACD_all = []\n",
    "for sample in tqdm(range(1,11)):\n",
    "    E_distort = pd.read_csv(f\"/home/chri6578/Documents/gttp/data/{dataset}/{distortion}_{sample}_ml_{dataset}.csv\")\n",
    "    ATD_all.append(f_ATD(E_real, E_distort, dataset))\n",
    "    ACD_all.append(f_ACD(E_real, E_distort, dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07999215425097446,\n",
       " 0.0800863057123326,\n",
       " 0.08023985374303896,\n",
       " 0.08016883669822782,\n",
       " 0.07945098730221332,\n",
       " 0.08018698750731404,\n",
       " 0.08007953780818729,\n",
       " 0.08002210789113358,\n",
       " 0.08020953375042707,\n",
       " 0.07983265602747991]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATD_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0011342193992763,\n",
       " 1.000871286720353,\n",
       " 1.0010465751729685,\n",
       " 1.001149686027448,\n",
       " 1.0007784869513214,\n",
       " 1.0009640864893847,\n",
       " 1.001453863048163,\n",
       " 1.0009537754039368,\n",
       " 1.0011342193992763,\n",
       " 1.001180619283792]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACD_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence95(metric_list):\n",
    "    # Step 1: Calculate the mean\n",
    "    mean = np.mean(metric_list)\n",
    "    # Step 2: Calculate the standard error of the mean (SEM)\n",
    "    sem = stats.sem(metric_list)\n",
    "    # Step 3: Find the critical value for a 95% confidence interval\n",
    "    confidence_level = 0.95\n",
    "    degrees_of_freedom = len(metric_list) - 1\n",
    "    critical_value = stats.t.ppf((1 + confidence_level) / 2, degrees_of_freedom)\n",
    "\n",
    "    # Step 4: Calculate the margin of error\n",
    "    margin_of_error = critical_value * sem\n",
    "    return mean, margin_of_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0800268960691329, 0.00016884958616460372)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence95(ATD_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0010666817895921, 0.00013569106443766214)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence95(ACD_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
